from vllm import LLM, SamplingParams
import json
import time
import os
import subprocess
import sys
from typing import List, Dict, Any, Optional, Tuple
import logging
from datasets import load_dataset
import numpy as np
import bz2
import tarfile
import requests
from urllib.parse import urljoin
import re
from collections import Counter
import string
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# Additional imports for enhanced evaluation
try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    print("Warning: rouge_score not available. Install with: pip install rouge-score")
    ROUGE_AVAILABLE = False

try:
    from bert_score import score as bert_score
    BERTSCORE_AVAILABLE = True
except ImportError:
    print("Warning: bert_score not available. Install with: pip install bert_score")
    BERTSCORE_AVAILABLE = False

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SelfRAGModel:
    """Exact Self-RAG model implementation following the original paper"""
    
    def __init__(self, 
                 model_path: str = "selfrag/selfrag_llama2_7b",
                 download_dir: str = "/gscratch/h2lab/akari/model_cache",
                 dtype: str = "half"):
        """Initialize Self-RAG model exactly as in original implementation"""
        self.model = LLM(model_path, download_dir=download_dir, dtype=dtype)
        # Exact sampling parameters from original Self-RAG
        self.sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=100, skip_special_tokens=False)

    def format_prompt(self, input_text, paragraph=None):
        """Format prompt exactly as in original Self-RAG implementation"""
        prompt = "### Instruction:\n{0}\n\n### Response:\n".format(input_text)
        if paragraph is not None:
            prompt += "[Retrieval]<paragraph>{0}</paragraph>".format(paragraph)
        return prompt

    def extract_utility_score(self, text: str) -> int:
        """Extract utility score from Self-RAG output tokens"""
        for i in range(5, 0, -1):
            if f'[Utility:{i}]' in text:
                return i
        return 0

    def extract_relevance(self, text: str) -> bool:
        """Extract relevance from Self-RAG output tokens"""
        return '[Relevant]' in text

    def extract_support(self, text: str) -> str:
        """Extract support level from Self-RAG output tokens"""
        if '[Fully supported]' in text:
            return 'fully_supported'
        elif '[Partially supported]' in text:
            return 'partially_supported'
        elif '[No support / Contradictory]' in text:
            return 'no_support'
        return 'unknown'

    def uses_retrieval(self, text: str) -> bool:
        """Check if model used retrieval during generation"""
        return '[Retrieve]' in text

class SelfRAGEvaluator:
    """Evaluator for Self-RAG following original paper evaluation"""
    
    def __init__(self):
        if ROUGE_AVAILABLE:
            self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        else:
            self.rouge_scorer = None
    
    def normalize_answer(self, s):
        """Normalize answer for evaluation (from SQuAD evaluation)"""
        def remove_articles(text):
            regex = re.compile(r'\b(a|an|the)\b', re.IGNORECASE)
            return re.sub(regex, ' ', text)
        
        def white_space_fix(text):
            return ' '.join(text.split())
        
        def remove_punc(text):
            exclude = set(string.punctuation)
            return ''.join(ch for ch in text if ch not in exclude)
        
        def lower(text):
            return text.lower()
        
        return white_space_fix(remove_articles(remove_punc(lower(s))))
    
    def exact_match_score(self, prediction, ground_truth):
        """Compute exact match score"""
        return (self.normalize_answer(prediction) == self.normalize_answer(ground_truth))
    
    def f1_score(self, prediction, ground_truth):
        """Compute F1 score (token-level)"""
        pred_tokens = self.normalize_answer(prediction).split()
        gold_tokens = self.normalize_answer(ground_truth).split()
        
        if not pred_tokens and not gold_tokens:
            return 1.0
        if not pred_tokens or not gold_tokens:
            return 0.0
        
        common = Counter(pred_tokens) & Counter(gold_tokens)
        num_same = sum(common.values())
        
        if num_same == 0:
            return 0.0
        
        precision = 1.0 * num_same / len(pred_tokens)
        recall = 1.0 * num_same / len(gold_tokens)
        f1 = (2 * precision * recall) / (precision + recall)
        
        return f1

    def evaluate_multiple_answers(self, prediction, ground_truths):
        """Evaluate against multiple possible ground truth answers"""
        if not ground_truths:
            return {'em': 0.0, 'f1': 0.0}
        
        # Take best score across all ground truths
        best_em = 0.0
        best_f1 = 0.0
        
        for gt in ground_truths:
            if not gt or not gt.strip():
                continue
                
            em = self.exact_match_score(prediction, gt)
            f1 = self.f1_score(prediction, gt)
            
            best_em = max(best_em, em)
            best_f1 = max(best_f1, f1)
        
        return {'em': best_em, 'f1': best_f1}

# Initialize global evaluator
evaluator = SelfRAGEvaluator()

def load_dataset_with_backup(options, sample_size):
    """Load dataset with multiple backup options"""
    ds = None
    for option in options:
        try:
            if len(option) == 2:
                dataset_name, split = option
                logger.info(f"Attempting to load {dataset_name}...")
                ds = load_dataset(dataset_name, split=split)
            elif len(option) == 3:
                dataset_name, config, split = option
                logger.info(f"Attempting to load {dataset_name} with config {config}...")
                ds = load_dataset(dataset_name, config, split=split)
            elif len(option) == 4:
                dataset_name, config, split, trust_code = option
                logger.info(f"Attempting to load {dataset_name} with config {config}...")
                ds = load_dataset(dataset_name, config, split=split, trust_remote_code=trust_code)
            
            logger.info(f"Successfully loaded {dataset_name}")
            break
        except Exception as e:
            logger.warning(f"Failed to load {dataset_name}: {e}")
            continue
    
    if ds is None:
        return None
        
    if sample_size < len(ds):
        ds = ds.select(range(sample_size))
        
    return ds

def run_natural_questions_benchmark(model, sample_size: int = 100):
    """Natural Questions - following Self-RAG paper evaluation"""
    logger.info(f"Running Natural Questions benchmark with {sample_size} samples...")
    
    # Backup download options for Natural Questions
    nq_options = [
        ("natural_questions", "default", "validation"),
        ("google-research-datasets/natural_questions", "default", "validation"),
        ("microsoft/ms_marco", "v2.1", "validation"),
        ("fever", "v1.0", "paper_dev")
    ]
    
    ds = load_dataset_with_backup(nq_options, sample_size)
    if ds is None:
        logger.error("Failed to load Natural Questions from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from Natural Questions")
    results = []
    
    for i, item in enumerate(ds):
        try:
            # Extract question and answers
            question = item.get('question', {}).get('text', '')
            annotations = item.get('annotations', [])
            document = item.get('document', {})
            
            # Extract context from document tokens
            tokens = document.get('tokens', [])
            if tokens:
                context_text = ' '.join([token.get('token', '') for token in tokens[:1000]])  
            else:
                context_text = ""
            
            # Extract answer spans
            answer_texts = []
            if annotations:
                for ann in annotations:
                    short_answers = ann.get('short_answers', [])
                    for sa in short_answers:
                        start_token = sa.get('start_token', 0)
                        end_token = sa.get('end_token', 0)
                        if start_token < len(tokens) and end_token <= len(tokens):
                            answer_text = ' '.join([tokens[j].get('token', '') for j in range(start_token, end_token)])
                            if answer_text.strip():
                                answer_texts.append(answer_text.strip())
            
            # Generate Self-RAG response with context
            if context_text.strip():
                prompt = model.format_prompt(question, context_text)
            else:
                prompt = model.format_prompt(question)
            
            start_time = time.time()
            pred = model.model.generate([prompt], model.sampling_params)[0]
            inference_time = time.time() - start_time
            
            response_text = pred.outputs[0].text
            
            # Evaluation
            if answer_texts:
                scores = evaluator.evaluate_multiple_answers(response_text, answer_texts)
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'natural_questions',
                'question': question,
                'response': response_text,
                'ground_truth_answers': answer_texts,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': inference_time,
                'tokens_generated': len(pred.outputs[0].token_ids),
                'utility_score': model.extract_utility_score(response_text),
                'is_relevant': model.extract_relevance(response_text),
                'support_level': model.extract_support(response_text),
                'uses_retrieval': model.uses_retrieval(response_text)
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} Natural Questions samples")
                
        except Exception as e:
            logger.error(f"Error processing Natural Questions item {i}: {e}")
            continue
    
    logger.info(f"Natural Questions benchmark completed with {len(results)} samples")
    return results

def run_trivia_qa_benchmark(model, sample_size: int = 100):
    """TriviaQA - following Self-RAG paper evaluation"""
    logger.info(f"Running TriviaQA benchmark with {sample_size} samples...")
    
    # Backup download options for TriviaQA
    trivia_options = [
        ("trivia_qa", "rc", "validation"),
        ("mandarjoshi/trivia_qa", "rc", "validation"), 
        ("microsoft/ms_marco", "v2.1", "validation"),
        ("fever", "v1.0", "paper_dev")
    ]
    
    ds = load_dataset_with_backup(trivia_options, sample_size)
    if ds is None:
        logger.error("Failed to load TriviaQA from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from TriviaQA")
    results = []
    
    for i, item in enumerate(ds):
        try:
            question = item.get('question', '')
            answer = item.get('answer', {})
            search_results = item.get('search_results', {})
            entity_pages = item.get('entity_pages', {})
            
            # Extract answer texts
            answer_texts = []
            if answer:
                value = answer.get('value', '')
                aliases = answer.get('aliases', [])
                if value:
                    answer_texts.append(value)
                answer_texts.extend(aliases)
            
            # Build context
            context_text = ""
            if search_results:
                search_contexts = search_results.get('search_context', [])
                if search_contexts:
                    context_text = "\n".join(search_contexts[:3])
            elif entity_pages:
                wiki_context = entity_pages.get('wiki_context', [])
                if wiki_context:
                    context_text = "\n".join(wiki_context[:3])
            
            # Generate response
            if context_text.strip():
                prompt = model.format_prompt(question, context_text)
            else:
                prompt = model.format_prompt(question)
            
            start_time = time.time()
            pred = model.model.generate([prompt], model.sampling_params)[0]
            inference_time = time.time() - start_time
            
            response_text = pred.outputs[0].text
            
            # Evaluation
            if answer_texts:
                scores = evaluator.evaluate_multiple_answers(response_text, answer_texts)
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'trivia_qa',
                'question': question,
                'response': response_text,
                'ground_truth_answers': answer_texts,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': inference_time,
                'tokens_generated': len(pred.outputs[0].token_ids),
                'utility_score': model.extract_utility_score(response_text),
                'is_relevant': model.extract_relevance(response_text),
                'support_level': model.extract_support(response_text),
                'uses_retrieval': model.uses_retrieval(response_text)
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} TriviaQA samples")
                
        except Exception as e:
            logger.error(f"Error processing TriviaQA item {i}: {e}")
            continue
    
    logger.info(f"TriviaQA benchmark completed with {len(results)} samples")
    return results

def run_hotpot_qa_benchmark(model, sample_size: int = 100):
    """HotpotQA - following Self-RAG paper evaluation"""
    logger.info(f"Running HotpotQA benchmark with {sample_size} samples...")
    
    # Backup download options for HotpotQA
    hotpot_options = [
        ("hotpot_qa", "distractor", "validation"),
        ("hotpotqa/hotpot_qa", "distractor", "validation"),
        ("microsoft/ms_marco", "v2.1", "validation"),
        ("fever", "v1.0", "paper_dev")
    ]
    
    ds = load_dataset_with_backup(hotpot_options, sample_size)
    if ds is None:
        logger.error("Failed to load HotpotQA from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from HotpotQA")
    results = []
    
    for i, item in enumerate(ds):
        try:
            question = item.get('question', '')
            answer = item.get('answer', '')
            context = item.get('context', [])
            supporting_facts = item.get('supporting_facts', [])
            level = item.get('level', 'unknown')
            type_question = item.get('type', 'unknown')
            
            # Build context from paragraphs
            context_text = ""
            if context:
                context_paragraphs = []
                for title, sentences in context:
                    if sentences:
                        paragraph_text = f"{title}: {' '.join(sentences)}"
                        context_paragraphs.append(paragraph_text)
                
                if context_paragraphs:
                    context_text = "\n".join(context_paragraphs[:5])
            
            # Generate response
            if context_text.strip():
                prompt = model.format_prompt(question, context_text)
            else:
                prompt = model.format_prompt(question)
            
            start_time = time.time()
            pred = model.model.generate([prompt], model.sampling_params)[0]
            inference_time = time.time() - start_time
            
            response_text = pred.outputs[0].text
            
            # Evaluation
            if answer:
                scores = evaluator.evaluate_multiple_answers(response_text, [answer])
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'hotpot_qa',
                'question': question,
                'response': response_text,
                'ground_truth_answer': answer,
                'level': level,
                'type': type_question,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': inference_time,
                'tokens_generated': len(pred.outputs[0].token_ids),
                'utility_score': model.extract_utility_score(response_text),
                'is_relevant': model.extract_relevance(response_text),
                'support_level': model.extract_support(response_text),
                'uses_retrieval': model.uses_retrieval(response_text),
                'num_context_paragraphs': len(context)
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} HotpotQA samples")
                
        except Exception as e:
            logger.error(f"Error processing HotpotQA item {i}: {e}")
            continue
    
    logger.info(f"HotpotQA benchmark completed with {len(results)} samples")
    return results

def run_squad_v2_benchmark(model, sample_size: int = 100):
    """SQuAD v2 - following Self-RAG paper evaluation"""
    logger.info(f"Running SQuAD v2 benchmark with {sample_size} samples...")
    
    # Backup download options for SQuAD v2
    squad_options = [
        ("rajpurkar/squad_v2", "validation"),
        ("squad_v2", "validation"),
        ("microsoft/ms_marco", "v2.1", "validation"),
        ("fever", "v1.0", "paper_dev")
    ]
    
    ds = load_dataset_with_backup(squad_options, sample_size)
    if ds is None:
        logger.error("Failed to load SQuAD v2 from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from SQuAD v2")
    results = []
    
    for i, item in enumerate(ds):
        try:
            question = item.get('question', '')
            context = item.get('context', '')
            answers = item.get('answers', {})
            squad_id = item.get('id', f'squad_{i}')
            
            # Check if question is answerable
            answer_texts = answers.get('text', []) if answers else []
            is_impossible = len(answer_texts) == 0
            
            # Generate response
            if context.strip():
                prompt = model.format_prompt(question, context)
            else:
                prompt = model.format_prompt(question)
            
            start_time = time.time()
            pred = model.model.generate([prompt], model.sampling_params)[0]
            inference_time = time.time() - start_time
            
            response_text = pred.outputs[0].text
            
            # Evaluation
            if not is_impossible and answer_texts:
                scores = evaluator.evaluate_multiple_answers(response_text, answer_texts)
            elif is_impossible:
                # Check if model correctly identifies as unanswerable
                no_answer_indicators = ["no answer", "cannot answer", "not provided", "unknown", "unanswerable"]
                detected_impossible = any(indicator in response_text.lower() for indicator in no_answer_indicators)
                scores = {'em': 1.0 if detected_impossible else 0.0, 'f1': 1.0 if detected_impossible else 0.0}
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'squad_v2',
                'id': squad_id,
                'question': question,
                'response': response_text,
                'ground_truth_answers': answer_texts,
                'is_impossible': is_impossible,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': inference_time,
                'tokens_generated': len(pred.outputs[0].token_ids),
                'utility_score': model.extract_utility_score(response_text),
                'is_relevant': model.extract_relevance(response_text),
                'support_level': model.extract_support(response_text),
                'uses_retrieval': model.uses_retrieval(response_text)
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} SQuAD v2 samples")
                
        except Exception as e:
            logger.error(f"Error processing SQuAD v2 item {i}: {e}")
            continue
    
    logger.info(f"SQuAD v2 benchmark completed with {len(results)} samples")
    return results

def run_crag_benchmark(model, sample_size: int = 100):
    """CRAG (Comprehensive RAG Benchmark) - following Self-RAG paper evaluation"""
    logger.info(f"Running CRAG benchmark with {sample_size} samples...")
    
    # Backup download options for CRAG
    crag_options = [
        ("facebook/crag", None, "dev", True),
        ("Quivr/CRAG", None, "dev", False),
        ("microsoft/ms_marco", "v2.1", "validation", False),
        ("fever", "v1.0", "paper_dev", False)
    ]
    
    ds = load_dataset_with_backup(crag_options, sample_size)
    if ds is None:
        logger.error("Failed to load CRAG from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples from CRAG")
    results = []
    
    for i, item in enumerate(ds):
        try:
            # Handle different dataset formats
            if 'interaction_id' in item:  # CRAG format
                interaction_id = item.get('interaction_id', f'crag_{i}')
                query = item.get('query', '')
                answer = item.get('answer', '')
                alt_ans = item.get('alt_ans', []) or []
                domain = item.get('domain', 'unknown')
                question_type = item.get('question_type', 'unknown')
                search_results = item.get('search_results', [])
            else:  # MS MARCO format
                interaction_id = f'marco_{i}'
                query = item.get('query', '')
                answer = ""
                alt_ans = item.get('answers', []) or []
                domain = 'general'
                question_type = 'factoid'
                search_results = item.get('passages', [])
            
            # Build context from search results
            context = None
            if search_results:
                contexts = []
                for result in search_results[:5]:
                    if 'page_snippet' in result:  # CRAG format
                        page_snippet = result.get('page_snippet', '')
                        page_name = result.get('page_name', '')
                        if page_snippet:
                            contexts.append(f"{page_name}: {page_snippet}")
                    elif 'passage_text' in result:  # MS MARCO format
                        passage_text = result.get('passage_text', '')
                        if passage_text:
                            contexts.append(passage_text)
                
                if contexts:
                    context = "\n".join(contexts)
            
            # Generate response
            if context:
                prompt = model.format_prompt(query, context)
            else:
                prompt = model.format_prompt(query)
            
            start_time = time.time()
            pred = model.model.generate([prompt], model.sampling_params)[0]
            inference_time = time.time() - start_time
            
            response_text = pred.outputs[0].text
            
            # Evaluation with multiple ground truths
            ground_truths = [answer] + alt_ans if answer else alt_ans
            ground_truths = [gt for gt in ground_truths if gt and gt.strip()]
            
            if ground_truths:
                scores = evaluator.evaluate_multiple_answers(response_text, ground_truths)
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'crag',
                'interaction_id': interaction_id,
                'query': query,
                'response': response_text,
                'ground_truth': answer,
                'alt_answers': alt_ans,
                'domain': domain,
                'question_type': question_type,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': inference_time,
                'tokens_generated': len(pred.outputs[0].token_ids),
                'utility_score': model.extract_utility_score(response_text),
                'is_relevant': model.extract_relevance(response_text),
                'support_level': model.extract_support(response_text),
                'uses_retrieval': model.uses_retrieval(response_text),
                'num_search_results': len(search_results)
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} CRAG samples")
            
        except Exception as e:
            logger.error(f"Error processing CRAG item {i}: {e}")
            continue
    
    logger.info(f"CRAG benchmark completed with {len(results)} samples")
    return results

def run_ragbench_benchmark(model, sample_size: int = 100):
    """RAGBench - following Self-RAG paper evaluation"""
    logger.info(f"Running RAGBench benchmark with {sample_size} samples...")
    
    # Backup download options for RAGBench evaluation
    ragbench_options = [
        ("microsoft/ms_marco", "v2.1", "validation"),
        ("BeIR/msmarco", None, "dev"),
        ("fever", "v1.0", "paper_dev")
    ]
    
    ds = load_dataset_with_backup(ragbench_options, sample_size)
    if ds is None:
        logger.error("Failed to load RAGBench from all backup sources")
        return []
    
    logger.info(f"Using {len(ds)} samples for RAGBench evaluation")
    results = []
    
    for i, item in enumerate(ds):
        try:
            query = item.get('query', '')
            passages = item.get('passages', [])
            answers = item.get('answers', [])
            wellFormedAnswers = item.get('wellFormedAnswers', [])
            
            # Build context from passages
            context_text = ""
            if passages:
                context_parts = []
                for passage in passages[:5]:
                    passage_text = passage.get('passage_text', '')
                    if passage_text:
                        context_parts.append(passage_text)
                
                if context_parts:
                    context_text = "\n".join(context_parts)
            
            # Get answer texts
            answer_texts = answers + wellFormedAnswers
            answer_texts = [ans for ans in answer_texts if ans and ans.strip()]
            
            # Generate response
            if context_text.strip():
                prompt = model.format_prompt(query, context_text)
            else:
                prompt = model.format_prompt(query)
            
            start_time = time.time()
            pred = model.model.generate([prompt], model.sampling_params)[0]
            inference_time = time.time() - start_time
            
            response_text = pred.outputs[0].text
            
            # Evaluation
            if answer_texts:
                scores = evaluator.evaluate_multiple_answers(response_text, answer_texts)
            else:
                scores = {'em': 0.0, 'f1': 0.0}
            
            results.append({
                'dataset': 'ragbench',
                'query': query,
                'response': response_text,
                'ground_truth_answers': answer_texts,
                'exact_match': scores['em'],
                'f1_score': scores['f1'],
                'inference_time': inference_time,
                'tokens_generated': len(pred.outputs[0].token_ids),
                'utility_score': model.extract_utility_score(response_text),
                'is_relevant': model.extract_relevance(response_text),
                'support_level': model.extract_support(response_text),
                'uses_retrieval': model.uses_retrieval(response_text),
                'num_passages': len(passages)
            })
            
            if (i + 1) % 10 == 0:
                logger.info(f"Processed {i + 1}/{len(ds)} RAGBench samples")
                
        except Exception as e:
            logger.error(f"Error processing RAGBench item {i}: {e}")
            continue
    
    logger.info(f"RAGBench benchmark completed with {len(results)} samples")
    return results

def create_evaluation_plots(results):
    """Create comprehensive plots for all evaluation data"""
    plt.style.use('seaborn-v0_8')
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Self-RAG Evaluation Results Across All Benchmarks', fontsize=16, fontweight='bold')
    
    # Prepare data for plotting
    datasets = []
    exact_matches = []
    f1_scores = []
    utility_scores = []
    retrieval_usage = []
    
    for dataset_name, dataset_results in results.items():
        if dataset_results.get('individual_results'):
            individual_results = dataset_results['individual_results']
            dataset_display_name = dataset_name.replace('_', ' ').title()
            
            # Collect all scores for this dataset
            em_scores = [r['exact_match'] for r in individual_results if 'exact_match' in r]
            f1_scores_data = [r['f1_score'] for r in individual_results if 'f1_score' in r]
            util_scores = [r['utility_score'] for r in individual_results if 'utility_score' in r]
            retrieval_data = [r['uses_retrieval'] for r in individual_results if 'uses_retrieval' in r]
            
            if em_scores:
                datasets.extend([dataset_display_name] * len(em_scores))
                exact_matches.extend(em_scores)
                f1_scores.extend(f1_scores_data)
                utility_scores.extend(util_scores)
                retrieval_usage.extend([1 if r else 0 for r in retrieval_data])
    
    # Plot 1: Exact Match Scores Distribution
    if exact_matches:
        ax1 = axes[0, 0]
        ax1.hist(exact_matches, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_xlabel('Exact Match Score')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Distribution of Exact Match Scores')
        ax1.grid(True, alpha=0.3)
    
    # Plot 2: F1 Scores Distribution
    if f1_scores:
        ax2 = axes[0, 1]
        ax2.hist(f1_scores, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        ax2.set_xlabel('F1 Score')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Distribution of F1 Scores')
        ax2.grid(True, alpha=0.3)
    
    # Plot 3: Utility Scores Distribution
    if utility_scores:
        ax3 = axes[0, 2]
        ax3.hist(utility_scores, bins=6, alpha=0.7, color='lightcoral', edgecolor='black')
        ax3.set_xlabel('Utility Score')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Distribution of Utility Scores')
        ax3.set_xticks(range(0, 6))
        ax3.grid(True, alpha=0.3)
    
    # Plot 4: Performance by Dataset (Box plot)
    if datasets and f1_scores:
        ax4 = axes[1, 0]
        dataset_f1_data = {}
        for dataset, f1 in zip(datasets, f1_scores):
            if dataset not in dataset_f1_data:
                dataset_f1_data[dataset] = []
            dataset_f1_data[dataset].append(f1)
        
        box_data = [scores for scores in dataset_f1_data.values()]
        box_labels = list(dataset_f1_data.keys())
        
        ax4.boxplot(box_data, labels=box_labels)
        ax4.set_ylabel('F1 Score')
        ax4.set_title('F1 Score Performance by Dataset')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(True, alpha=0.3)
    
    # Plot 5: Retrieval Usage by Dataset
    if datasets and retrieval_usage:
        ax5 = axes[1, 1]
        dataset_retrieval_data = {}
        for dataset, retrieval in zip(datasets, retrieval_usage):
            if dataset not in dataset_retrieval_data:
                dataset_retrieval_data[dataset] = []
            dataset_retrieval_data[dataset].append(retrieval)
        
        retrieval_means = [np.mean(scores) for scores in dataset_retrieval_data.values()]
        dataset_names = list(dataset_retrieval_data.keys())
        
        bars = ax5.bar(dataset_names, retrieval_means, color='orange', alpha=0.7)
        ax5.set_ylabel('Retrieval Usage Rate')
        ax5.set_title('Retrieval Usage by Dataset')
        ax5.tick_params(axis='x', rotation=45)
        ax5.set_ylim(0, 1)
        
        # Add percentage labels on bars
        for bar, mean_val in zip(bars, retrieval_means):
            height = bar.get_height()
            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{mean_val:.1%}', ha='center', va='bottom')
    
    # Plot 6: Scatter plot of F1 vs Utility
    if f1_scores and utility_scores:
        ax6 = axes[1, 2]
        scatter = ax6.scatter(utility_scores, f1_scores, alpha=0.6, c='purple')
        ax6.set_xlabel('Utility Score')
        ax6.set_ylabel('F1 Score')
        ax6.set_title('F1 Score vs Utility Score')
        ax6.grid(True, alpha=0.3)
        
        # Add correlation coefficient
        if len(f1_scores) > 1 and len(utility_scores) > 1:
            correlation = np.corrcoef(utility_scores, f1_scores)[0, 1]
            ax6.text(0.05, 0.95, f'Correlation: {correlation:.3f}', 
                    transform=ax6.transAxes, bbox=dict(boxstyle="round", facecolor='wheat'))
    
    plt.tight_layout()
    
    # Save the plot
    timestamp = int(time.time())
    plot_filename = f"selfrag_evaluation_plots_{timestamp}.png"
    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
    logger.info(f"Evaluation plots saved to {plot_filename}")
    
    # Show the plot
    plt.show()
    
    return plot_filename

def save_raw_data_only(results, filename):
    """Save only the raw evaluation data without any statistical summaries"""
    try:
        # Extract only individual results from each benchmark
        raw_data = {}
        total_samples = 0
        
        for benchmark_name, benchmark_data in results.items():
            individual_results = benchmark_data.get('individual_results', [])
            if individual_results:
                raw_data[benchmark_name] = individual_results
                total_samples += len(individual_results)
        
        # Create minimal output structure
        output_data = {
            'evaluation_metadata': {
                'model': 'Self-RAG (LLaMA2-7B)',
                'total_samples_evaluated': total_samples,
                'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'benchmarks_completed': len(raw_data)
            },
            'raw_evaluation_results': raw_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Raw evaluation data saved to {filename}")
        logger.info(f"Total samples: {total_samples}")
        
        return output_data
        
    except Exception as e:
        logger.error(f"Error saving raw data to {filename}: {e}")
        return None

def main():
    """Main function to run all Self-RAG benchmarks and output raw data only"""
    print("="*70)
    print("SELF-RAG EVALUATION - RAW DATA OUTPUT")
    print("No statistical summaries - just the evaluation scores")
    print("="*70)
    
    # Initialize Self-RAG model
    logger.info("Initializing Self-RAG model...")
    try:
        model = SelfRAGModel(
            model_path="selfrag/selfrag_llama2_7b",
            download_dir="/gscratch/h2lab/akari/model_cache",
            dtype="half"
        )
        logger.info("Self-RAG model initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize Self-RAG model: {e}")
        return
    
    # Configuration - using full datasets
    sample_size = 1000  # Increased for more complete evaluation
    results = {}
    
    # Define benchmarks
    benchmarks = [
        ("Natural Questions", run_natural_questions_benchmark),
        ("TriviaQA", run_trivia_qa_benchmark), 
        ("HotpotQA", run_hotpot_qa_benchmark),
        ("SQuAD v2", run_squad_v2_benchmark),
        ("CRAG", run_crag_benchmark),
        ("RAGBench", run_ragbench_benchmark)
    ]
    
    logger.info(f"Running {len(benchmarks)} benchmarks with {sample_size} samples each...")
    
    # Run each benchmark
    for benchmark_name, benchmark_func in benchmarks:
        print(f"\n{'='*60}")
        print(f"RUNNING: {benchmark_name}")
        print(f"{'='*60}")
        
        try:
            start_time = time.time()
            benchmark_results = benchmark_func(model, sample_size=sample_size)
            end_time = time.time()
            
            if benchmark_results:
                results[benchmark_name.lower().replace(' ', '_')] = {
                    'individual_results': benchmark_results,
                    'total_samples': len(benchmark_results),
                    'execution_time': end_time - start_time
                }
                
                logger.info(f"{benchmark_name} completed: {len(benchmark_results)} samples in {end_time - start_time:.2f}s")
                
            else:
                logger.warning(f"{benchmark_name} returned no results")
                results[benchmark_name.lower().replace(' ', '_')] = {
                    'individual_results': [],
                    'total_samples': 0,
                    'execution_time': end_time - start_time,
                    'status': 'failed'
                }
                
        except Exception as e:
            logger.error(f"Error running {benchmark_name}: {e}")
            results[benchmark_name.lower().replace(' ', '_')] = {
                'individual_results': [],
                'total_samples': 0,
                'execution_time': 0,
                'status': 'error',
                'error_message': str(e)
            }
    
    # Save raw data only
    timestamp = int(time.time())
    raw_filename = f"selfrag_raw_data_{timestamp}.json"
    raw_data = save_raw_data_only(results, raw_filename)
    
    # Create visualization
    print(f"\n{'='*60}")
    print("CREATING EVALUATION PLOTS...")
    print(f"{'='*60}")
    
    try:
        plot_filename = create_evaluation_plots(results)
        print(f"Plots saved to: {plot_filename}")
    except Exception as e:
        logger.error(f"Error creating plots: {e}")
    
    # Final summary
    print(f"\n{'='*70}")
    print("SELF-RAG EVALUATION COMPLETE")
    print(f"{'='*70}")
    
    total_samples = sum(r.get('total_samples', 0) for r in results.values())
    successful_benchmarks = sum(1 for r in results.values() if r.get('total_samples', 0) > 0)
    
    print(f"Benchmarks completed: {successful_benchmarks}/6")
    print(f"Total samples evaluated: {total_samples}")
    print(f"Raw data saved to: {raw_filename}")
    
    if raw_data:
        print(f"\nData breakdown:")
        for benchmark, data in raw_data['raw_evaluation_results'].items():
            print(f"  {benchmark.replace('_', ' ').title()}: {len(data)} samples")
    
    return results

if __name__ == "__main__":
    # Set up environment
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    
    print("SELF-RAG EVALUATION - STREAMLINED VERSION")
    print("Raw data output + visualization")
    print("=" * 70)
    
    # Pre-flight checks
    print("Running pre-flight checks...")
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"GPU Available: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("No GPU detected - evaluation will be slow!")
    except ImportError:
        print("PyTorch not available for GPU check")
    
    # Check required packages
    required_packages = ['vllm', 'datasets', 'transformers', 'torch', 'matplotlib', 'seaborn']
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"{package} available")
        except ImportError:
            missing_packages.append(package)
            print(f"{package} missing")
    
    if missing_packages:
        print(f"\nInstall missing packages:")
        print(f"pip install {' '.join(missing_packages)}")
        sys.exit(1)
    
    print("All checks passed! Starting evaluation...")
    
    # Run the evaluation
    try:
        results = main()
        print("\nEVALUATION COMPLETED SUCCESSFULLY!")
        
    except KeyboardInterrupt:
        print("\nEvaluation interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)
